# Scaling Laws in LLM
This repository compiles a variety of scaling laws related to Large Language Models (LLMs), covering aspects such as model size, the volume of pretraining tokens, finetuning samples, and positional embeddings.

## Laws related to Pretraining
- [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)  
- [Scaling Data-Constrained Language Models (NeurIPS2023)](https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d89448b63ce1e2e8dc7af72c984c196-Abstract-Conference.html)
  
## Laws related to Finetuning
- [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method (ICLR2024)](https://openreview.net/pdf?id=5HCnKDeTws)  

## Laws related to Positional Embedding
- [Scaling Laws of RoPE-based Extrapolation (ICLR2024)](https://arxiv.org/abs/2310.05209)

## Misc
- [Scaling Laws for Generative Mixed-Modal Language Models (PMLR)](https://proceedings.mlr.press/v202/aghajanyan23a.html)